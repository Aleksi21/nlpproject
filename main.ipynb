{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a script that calls the Open Alex API with the query “climate change mitigation”. We focus on\n",
    "the search outcome at different time period:1850-1900; 1900-1920; 1921-1940; 1941-1960; 1961-\n",
    "1980; 1981-2000; 2001-2012; 2013-2024. For each time period, select the 30 results that have the\n",
    "highest citations. For each outcome create a metadata, which include the title of the paper, its\n",
    "abstract, the corresponding topic in the OpenAlex category, list of authors and their institution. For\n",
    "each time period, create a dataframe gathering all the thirty datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from collections import Counter\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "query = \"climate change mitigation\"\n",
    "time_periods = [\n",
    "    (1850, 1900),\n",
    "    (1900, 1920),\n",
    "    (1921, 1940),\n",
    "    (1941, 1960),\n",
    "    (1961, 1980),\n",
    "    (1981, 2000),\n",
    "    (2001, 2012),\n",
    "    (2013, 2024)\n",
    "]\n",
    "\n",
    "MAX_REQUESTS_PER_SECOND = 10\n",
    "REQUEST_DELAY = 1 / MAX_REQUESTS_PER_SECOND\n",
    "\n",
    "def fetch_openalex_data(start_year, end_year, query, limit=30):\n",
    "    \"\"\"\n",
    "    Fetch data from OpenAlex API for a specific time period with pagination.\n",
    "    \"\"\"\n",
    "    collected_results = []\n",
    "    page = 1\n",
    "    per_page = 200\n",
    "    \n",
    "    while len(collected_results) < limit:\n",
    "        url = (f\"{BASE_URL}?search={query}&filter=from_publication_date:{start_year}-01-01,\"\n",
    "               f\"to_publication_date:{end_year}-12-31&sort=cited_by_count:desc&per_page={per_page}&page={page}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get(\"results\", [])\n",
    "            if not results:\n",
    "                print(f\"No more results for period {start_year}-{end_year}. Collected {len(collected_results)} articles.\")\n",
    "                break\n",
    "            \n",
    "            collected_results.extend(results[:limit - len(collected_results)])\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} for period {start_year}-{end_year}\")\n",
    "            break\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    print(f\"Fetched {len(collected_results)} articles for period {start_year}-{end_year}\")\n",
    "    return collected_results[:limit]\n",
    "\n",
    "def reconstruct_abstract(abstract_inverted_index):\n",
    "    \"\"\"\n",
    "    Reconstructs the abstract from abstract_inverted_index.\n",
    "    \"\"\"\n",
    "    if not abstract_inverted_index:\n",
    "        return \"\"\n",
    "    \n",
    "    max_position = max([max(positions) for positions in abstract_inverted_index.values()])\n",
    "    abstract_words = [None] * (max_position + 1)\n",
    "    \n",
    "    for word, positions in abstract_inverted_index.items():\n",
    "        for position in positions:\n",
    "            abstract_words[position] = word\n",
    "    \n",
    "    return \" \".join(filter(None, abstract_words))\n",
    "\n",
    "def process_metadata(results):\n",
    "    \"\"\"\n",
    "    Process and extract fields from API results.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for item in results:\n",
    "        abstract_text = reconstruct_abstract(item.get(\"abstract_inverted_index\", {}))\n",
    "        \n",
    "        paper_info = {\n",
    "            \"Title\": item.get(\"title\", \"\"),\n",
    "            \"Abstract\": abstract_text,\n",
    "            \"Category\": item.get(\"concepts\", [{}])[0].get(\"display_name\", \"\") if item.get(\"concepts\") else \"\",\n",
    "            \"Authors\": [author.get(\"author\", {}).get(\"display_name\", \"\") for author in item.get(\"authorships\", [])],\n",
    "            \"Institutions\": [\n",
    "                author.get(\"institutions\", [{}])[0].get(\"display_name\", \"\")\n",
    "                if author.get(\"institutions\") else \"\"\n",
    "                for author in item.get(\"authorships\", [])\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        if paper_info[\"Title\"]:\n",
    "            data.append(paper_info)\n",
    "    \n",
    "    df = pd.DataFrame(data[:30])\n",
    "    if len(df) < 30:\n",
    "        empty_rows = pd.DataFrame([{\"Title\": \"\", \"Abstract\": \"\", \"Category\": \"\", \"Authors\": [], \"Institutions\": []} for _ in range(30 - len(df))])\n",
    "        df = pd.concat([df, empty_rows], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fetch_and_process_period(period):\n",
    "    start_year, end_year = period\n",
    "    print(f\"Fetching data for period: {start_year}-{end_year}\")\n",
    "    results = fetch_openalex_data(start_year, end_year, query)\n",
    "    if results:\n",
    "        return process_metadata(results)\n",
    "    else:\n",
    "        print(f\"No data retrieved for period: {start_year}-{end_year}\")\n",
    "        return pd.DataFrame()\n",
    "all_dataframes = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    future_to_period = {executor.submit(fetch_and_process_period, period): period for period in time_periods}\n",
    "\n",
    "    for future in as_completed(future_to_period):\n",
    "        period = future_to_period[future]\n",
    "        start_year, end_year = period\n",
    "        period_key = f\"{start_year}-{end_year}\"\n",
    "        \n",
    "        try:\n",
    "            df = future.result()\n",
    "            all_dataframes[period_key] = df\n",
    "            print(f\"Data for period {period_key} saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for period {period_key}: {e}\")\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    if not df.empty:\n",
    "        df.to_csv(f\"climate_change_mitigation_{period}.csv\", index=False)\n",
    "        print(f\"Saved data for period {period} to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_abstract_availability(dataframes):\n",
    "    availability_data = {}\n",
    "    for period, df in dataframes.items():\n",
    "        total_entries = len(df)\n",
    "        available_abstracts = df[\"Abstract\"].apply(lambda x: x != \"\").sum()\n",
    "        if total_entries > 0:\n",
    "            percent_available = round((available_abstracts / total_entries) * 100, 2)\n",
    "        else:\n",
    "            percent_available = 0\n",
    "        \n",
    "        availability_data[period] = percent_available\n",
    "        print(f\"Period: {period}, Total Entries: {total_entries}, Available Abstracts: {available_abstracts}, Percentage: {percent_available}%\")\n",
    "    \n",
    "    return availability_data\n",
    "\n",
    "abstract_availability = calculate_abstract_availability(all_dataframes)\n",
    "sorted_periods = sorted(abstract_availability.keys(), key=lambda x: int(x.split('-')[0]))\n",
    "time_periods = sorted_periods\n",
    "percentages = [abstract_availability[period] for period in time_periods]\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.plot(time_periods, percentages, 'o-', color='royalblue', linewidth=2, markersize=8, label=\"Abstract Availability\")\n",
    "\n",
    "for i, (period, percentage) in enumerate(zip(time_periods, percentages)):\n",
    "    plt.text(period, percentage + 1, f\"{percentage:.1f}%\", ha='center', va='bottom', fontsize=12, color='darkblue')\n",
    "\n",
    "plt.title(\"Percentage of Abstracts Available per Period\", fontsize=16)\n",
    "plt.xlabel(\"Time Period\", fontsize=14)\n",
    "plt.ylabel(\"Percentage of Available Abstracts\", fontsize=14)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(72, max(percentages) + 5)\n",
    "plt.savefig(\"abstract_availability_chart.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './'\n",
    "all_files = glob.glob(folder_path + \"climate_change_mitigation_*.csv\")\n",
    "abstract_lengths = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    if 'Abstract' in df.columns:\n",
    "        abstract_lengths.extend(df['Abstract'].dropna().apply(lambda x: len(x.split())))\n",
    "        \n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.hist(abstract_lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Abstract Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Abstract Lengths Across All CSV Files (Same Directory)')\n",
    "plt.grid(axis='y')\n",
    "plt.savefig(\"abstract_length_distribution.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a script that scrutinizes the categories associated to documents of the same time period, and\n",
    "output the percentage of each category in the thirty outcomes. Then, for a given category that\n",
    "appears in more than one time period, draw a plot that shows the evolution of each category across\n",
    "different time periods (Trace the categories on the same plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_distribution(df):\n",
    "    \"\"\"\n",
    "    Function to calculate category distribution for each period.\n",
    "    \"\"\"\n",
    "    category_counts = df[\"Category\"].value_counts(normalize=True) * 100\n",
    "    return category_counts\n",
    "\n",
    "category_distributions = {}\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    category_distribution = calculate_category_distribution(df)\n",
    "    category_distributions[period] = category_distribution\n",
    "\n",
    "sorted_periods = sorted(category_distributions.keys(), key=lambda x: int(x.split('-')[0]))\n",
    "all_categories = [set(distribution.index) for distribution in category_distributions.values()]\n",
    "category_counter = Counter(category for distribution in all_categories for category in distribution)\n",
    "multi_period_categories = {category for category, count in category_counter.items() if count > 1}\n",
    "\n",
    "for period in sorted_periods:\n",
    "    distribution = category_distributions[period]\n",
    "    print(f\"\\nPeriod: {period}\")\n",
    "    print(distribution, \"\\n\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "markers = cycle(['o', 's', 'D', '^', 'v', '<', '>', 'p', '*', 'h', 'X', 'P'])\n",
    "styles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "for category in multi_period_categories:\n",
    "    time_periods = []\n",
    "    percentages = []\n",
    "    \n",
    "    for period in sorted_periods:\n",
    "        distribution = category_distributions[period]\n",
    "        time_periods.append(period)\n",
    "        percentages.append(distribution.get(category, 0))\n",
    "    \n",
    "    plt.plot(\n",
    "        time_periods, \n",
    "        percentages, \n",
    "        linestyle=next(styles),\n",
    "        marker=next(markers), \n",
    "        label=category, \n",
    "        linewidth=1.5\n",
    "    )\n",
    "    for period, percentage in zip(time_periods, percentages):\n",
    "        if percentage > 0:\n",
    "            plt.annotate(\n",
    "                f\"{percentage:.1f}%\", \n",
    "                (period, percentage), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0, 5), \n",
    "                ha='center', \n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "plt.title(\"Evolution of Categories Over Time\", fontsize=16)\n",
    "plt.xlabel(\"Time Period\", fontsize=14)\n",
    "plt.ylabel(\"Percentage of Articles\", fontsize=14)\n",
    "plt.legend(title=\"Categories\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='medium', title_fontsize='medium')\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"category_evolution_over_time.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now we want to explore the titles of the documents. For this purpose, suggest a script that gathers\n",
    "all titles of the same time period, then uses DistilBERT to generate an embedding vector, then uses\n",
    "the t-SNE Visualizing Word Vectors with t-SNE | Kaggle, to project the embedding into a 2D space,\n",
    "so that all titles of the same time period are represented as a single 2D point. Finally, draw a plot\n",
    "showing the evolution of the titles across various time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "def get_batch_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of texts using DistilBERT.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def calculate_average_embedding(df):\n",
    "    \"\"\"\n",
    "    Calculate the average embedding for all titles within a period.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    batch_size = 16\n",
    "    titles = df[\"Title\"].dropna().tolist()\n",
    "\n",
    "    for i in range(0, len(titles), batch_size):\n",
    "        batch_titles = titles[i:i + batch_size]\n",
    "        batch_embeddings = get_batch_embeddings(batch_titles)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "period_embeddings = {}\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    print(f\"Calculating embeddings for period: {period}\")\n",
    "    avg_embedding = calculate_average_embedding(df)\n",
    "    period_embeddings[period] = avg_embedding\n",
    "\n",
    "period_labels = list(period_embeddings.keys())\n",
    "embedding_matrix = np.vstack(list(period_embeddings.values()))\n",
    "\n",
    "tsne = TSNE(n_components=2, init='random', random_state=42, perplexity=3)\n",
    "tsne_results = tsne.fit_transform(embedding_matrix)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "for i, period in enumerate(period_labels):\n",
    "    plt.scatter(tsne_results[i, 0], tsne_results[i, 1], label=period, s=100)\n",
    "    plt.text(tsne_results[i, 0] + 0.1, tsne_results[i, 1] + 0.1, period, fontsize=9)\n",
    "\n",
    "for i in range(len(tsne_results) - 1):\n",
    "    plt.plot([tsne_results[i, 0], tsne_results[i + 1, 0]], \n",
    "             [tsne_results[i, 1], tsne_results[i + 1, 1]], 'k--', alpha=0.5)\n",
    "\n",
    "plt.title(\"Evolution of Title Embeddings Across Time Periods\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend(title=\"Time Periods\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"title_embedding_evolution.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Repeat the process in 3) when considering all abstracts of documents falling under the same time \n",
    "period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "def get_batch_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of texts using DistilBERT.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def calculate_average_embedding_abstracts(df):\n",
    "    \"\"\"\n",
    "    Calculate the average embedding for all abstracts within a period.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    batch_size = 8\n",
    "    abstracts = df[\"Abstract\"].dropna().tolist()\n",
    "\n",
    "    for i in range(0, len(abstracts), batch_size):\n",
    "        batch_abstracts = abstracts[i:i + batch_size]\n",
    "        batch_embeddings = get_batch_embeddings(batch_abstracts)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "period_embeddings_abstracts = {}\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    print(f\"Calculating embeddings for abstracts in period: {period}\")\n",
    "    avg_embedding = calculate_average_embedding_abstracts(df)\n",
    "    period_embeddings_abstracts[period] = avg_embedding\n",
    "\n",
    "period_labels_abstracts = list(period_embeddings_abstracts.keys())\n",
    "embedding_matrix_abstracts = np.vstack(list(period_embeddings_abstracts.values()))\n",
    "tsne_abstracts = TSNE(n_components=2, init='random', random_state=42, perplexity=5)\n",
    "tsne_results_abstracts = tsne_abstracts.fit_transform(embedding_matrix_abstracts)\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "for i, period in enumerate(period_labels_abstracts):\n",
    "    plt.scatter(tsne_results_abstracts[i, 0], tsne_results_abstracts[i, 1], label=period, s=100)\n",
    "    plt.text(tsne_results_abstracts[i, 0] + 0.1, tsne_results_abstracts[i, 1] + 0.1, period, fontsize=9)\n",
    "\n",
    "for i in range(len(tsne_results_abstracts) - 1):\n",
    "    plt.plot([tsne_results_abstracts[i, 0], tsne_results_abstracts[i + 1, 0]], \n",
    "             [tsne_results_abstracts[i, 1], tsne_results_abstracts[i + 1, 1]], 'k--', alpha=0.5)\n",
    "\n",
    "plt.title(\"Evolution of Abstract Embeddings Across Time Periods\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend(title=\"Time Periods\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now we want to comprehend the diversity of documents falling on the same period. For this \n",
    "purpose, use the DistilBERT embedding for each abstract, followed by t-SNE decomposition for 2D \n",
    "representation, and draw a plot for each time period highlighting the distribution of abstracts in the \n",
    "2D plot (each abstract corresponds to a single point), at the same time, draw the distribution of \n",
    "categories over the same time period. Comment whether some analogies between embedding-based \n",
    "representation and category-based representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of texts using DistilBERT.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def get_abstract_embeddings(df):\n",
    "    \"\"\"\n",
    "    Get embeddings for all abstracts in the dataframe in batches and track their categories.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    categories = df[\"Category\"].values\n",
    "    batch_size = 8\n",
    "    abstracts = df[\"Abstract\"].dropna().tolist()\n",
    "\n",
    "    for i in range(0, len(abstracts), batch_size):\n",
    "        batch_abstracts = abstracts[i:i + batch_size]\n",
    "        batch_embeddings = get_batch_embeddings(batch_abstracts)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings, categories[:len(embeddings)]\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    print(f\"Processing period: {period}\")\n",
    "    \n",
    "    embeddings, categories = get_abstract_embeddings(df)\n",
    "    pca = PCA(n_components=20, random_state=42)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "    tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=5)\n",
    "    tsne_results = tsne.fit_transform(reduced_embeddings)\n",
    "\n",
    "    category_counts = Counter(categories)\n",
    "    total_abstracts = len(categories)\n",
    "    category_percentages = {cat: (count / total_abstracts) * 100 for cat, count in category_counts.items()}\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 14), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    unique_categories = list(category_counts.keys())\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_categories)))\n",
    "    color_map = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "    for i, (x, y) in enumerate(tsne_results):\n",
    "        category = categories[i]\n",
    "        ax1.scatter(x, y, color=color_map[category], s=50, alpha=0.7)\n",
    "\n",
    "    for category, color in color_map.items():\n",
    "        ax1.scatter([], [], color=color, label=category, s=50)\n",
    "\n",
    "    ax1.set_title(f\"Abstract Embedding Distribution for {period}\")\n",
    "    ax1.set_xlabel(\"t-SNE Dimension 1\")\n",
    "    ax1.set_ylabel(\"t-SNE Dimension 2\")\n",
    "    ax1.legend(title=\"Categories\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True)\n",
    "    ax2.bar(\n",
    "        category_percentages.keys(),\n",
    "        category_percentages.values(),\n",
    "        color=[color_map[cat] for cat in category_percentages.keys()],\n",
    "        alpha=0.7\n",
    "    )\n",
    "    ax2.set_title(f\"Category Distribution for {period}\")\n",
    "    ax2.set_ylabel(\"Percentage of Abstracts\")\n",
    "    ax2.set_xticks(range(len(category_percentages)))\n",
    "    ax2.set_xticklabels(category_percentages.keys(), rotation=45, ha=\"right\")\n",
    "\n",
    "    plt.savefig(f\"abstract_embedding_distribution_{period}.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We want to assess the compatibility of category result outputted by OpenAlex with abstract topic\n",
    "based analysis. For this purpose, given the set of all abstracts falling on the same time scale, with the \n",
    "corresponding embedding as performed in Task 4), create a DistilBERT embedding vector for each \n",
    "category title as described in OpenAlex, and then write a program that computes the cosine \n",
    "similarity between each topic title embedding and the abstracts embedding, so that the category that \n",
    "yields the highest cosine similarity score will be assigned to it. Comment whether the compatibility \n",
    "is feasible when taking the overall set of abstracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate a DistilBERT embedding for a single text input.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "def get_category_embeddings(unique_categories):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of unique category titles.\n",
    "    \"\"\"\n",
    "    category_embeddings = {}\n",
    "    for category in unique_categories:\n",
    "        category_embedding = get_embedding(category)\n",
    "        category_embeddings[category] = category_embedding\n",
    "    return category_embeddings\n",
    "\n",
    "def assign_category_to_abstracts(abstract_embeddings, category_embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between each abstract embedding and each category embedding,\n",
    "    and assign the most similar category to each abstract.\n",
    "    \"\"\"\n",
    "    assigned_categories = []\n",
    "    category_names = list(category_embeddings.keys())\n",
    "    category_vectors = np.array(list(category_embeddings.values()))\n",
    "    \n",
    "    for abstract_embedding in abstract_embeddings:\n",
    "        similarities = cosine_similarity([abstract_embedding], category_vectors)[0]\n",
    "        best_category_index = np.argmax(similarities)\n",
    "        assigned_category = category_names[best_category_index]\n",
    "        assigned_categories.append((assigned_category, similarities[best_category_index]))\n",
    "\n",
    "    return assigned_categories\n",
    "\n",
    "compatibility_results = {}\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    print(f\"\\nProcessing period: {period}\")\n",
    "    abstract_embeddings, actual_categories = get_abstract_embeddings(df)\n",
    "    \n",
    "    unique_categories = df[\"Category\"].dropna().unique()\n",
    "    category_embeddings = get_category_embeddings(unique_categories)\n",
    "    assigned_categories = assign_category_to_abstracts(abstract_embeddings, category_embeddings)\n",
    "    compatible_count = sum(1 for i, actual_cat in enumerate(actual_categories) if actual_cat == assigned_categories[i][0])\n",
    "    compatibility_ratio = compatible_count / len(abstract_embeddings) * 100\n",
    "    compatibility_results[period] = compatibility_ratio\n",
    "\n",
    "    print(f\"Compatibility of OpenAlex categories with topic-based assignment for period {period}:\")\n",
    "    print(f\"Percentage of abstracts where assigned category matches OpenAlex category: {compatibility_ratio:.2f}%\")\n",
    "    print(\"Examples of assigned categories with highest cosine similarity:\")\n",
    "    for i in range(min(5, len(assigned_categories))):\n",
    "        print(f\"Abstract {i+1}: Actual Category = {actual_categories[i]}, Assigned Category = {assigned_categories[i][0]}, Similarity Score = {assigned_categories[i][1]:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "print(\"\\nSummary of Compatibility Ratios Across Periods:\")\n",
    "for period, ratio in compatibility_results.items():\n",
    "    print(f\"{period}: {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Repeat 6) when considering individual abstracts instead of the concatenated list of all abstracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generate a DistilBERT embedding for a single text input.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy().flatten()\n",
    "\n",
    "def get_category_embeddings(unique_categories):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of unique category titles.\n",
    "    \"\"\"\n",
    "    category_embeddings = {}\n",
    "    for category in unique_categories:\n",
    "        category_embedding = get_embedding(category)\n",
    "        category_embeddings[category] = category_embedding\n",
    "    return category_embeddings\n",
    "\n",
    "def assign_category_to_individual_abstract(abstract_embedding, category_embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between a single abstract embedding and each category embedding,\n",
    "    and assign the category with the highest similarity score.\n",
    "    \"\"\"\n",
    "    category_names = list(category_embeddings.keys())\n",
    "    category_vectors = np.array(list(category_embeddings.values()))\n",
    "    similarities = cosine_similarity([abstract_embedding], category_vectors)[0]\n",
    "    best_category_index = np.argmax(similarities)\n",
    "    assigned_category = category_names[best_category_index]\n",
    "    return assigned_category, similarities[best_category_index]\n",
    "\n",
    "compatibility_results_individual = {}\n",
    "\n",
    "for period, df in all_dataframes.items():\n",
    "    print(f\"\\nProcessing period: {period}\")\n",
    "    abstract_embeddings = []\n",
    "    actual_categories = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notnull(row[\"Abstract\"]):\n",
    "            abstract_embeddings.append(get_embedding(row[\"Abstract\"]))\n",
    "            actual_categories.append(row[\"Category\"])\n",
    "\n",
    "    unique_categories = df[\"Category\"].dropna().unique()\n",
    "    category_embeddings = get_category_embeddings(unique_categories)\n",
    "    assigned_categories = []\n",
    "\n",
    "    for i, abstract_embedding in enumerate(abstract_embeddings):\n",
    "        assigned_category, similarity_score = assign_category_to_individual_abstract(abstract_embedding, category_embeddings)\n",
    "        assigned_categories.append((assigned_category, similarity_score))\n",
    "    \n",
    "    compatible_count = sum(1 for i, actual_cat in enumerate(actual_categories) if actual_cat == assigned_categories[i][0])\n",
    "    compatibility_ratio = compatible_count / len(abstract_embeddings) * 100 if abstract_embeddings else 0\n",
    "    compatibility_results_individual[period] = compatibility_ratio\n",
    "\n",
    "    print(f\"Compatibility of OpenAlex categories with topic-based assignment for period {period} (individual abstracts):\")\n",
    "    print(f\"Percentage of abstracts where assigned category matches OpenAlex category: {compatibility_ratio:.2f}%\")\n",
    "    print(\"Examples of assigned categories with highest cosine similarity:\")\n",
    "    sorted_examples = sorted(assigned_categories, key=lambda x: x[1], reverse=True)\n",
    "    for i in range(min(5, len(sorted_examples))):\n",
    "        actual_category = actual_categories[i]\n",
    "        assigned_category, similarity_score = sorted_examples[i]\n",
    "        print(f\"Abstract {i+1}: Actual Category = {actual_category}, Assigned Category = {assigned_category}, Similarity Score = {similarity_score:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nSummary of Compatibility Ratios Across Periods (Individual Abstracts):\")\n",
    "for period, ratio in compatibility_results_individual.items():\n",
    "    print(f\"{period}: {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Repeat 6) and 7) when word2vec was used instead of DistilBERT embeddings.  Summarize the \n",
    "findings in a table highlighting the performance of various embeddings and data processing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "np.random.seed(11)\n",
    "\n",
    "corpus = [abstract.split() for df in all_dataframes.values() for abstract in df[\"Abstract\"].dropna()]\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "\n",
    "def get_word2vec_embedding(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def get_category_embeddings_word2vec(unique_categories, model):\n",
    "    category_embeddings = {}\n",
    "    for category in unique_categories:\n",
    "        category_embedding = get_word2vec_embedding(category, model)\n",
    "        category_embeddings[category] = category_embedding\n",
    "    return category_embeddings\n",
    "\n",
    "def assign_category(abstract_embedding, category_embeddings):\n",
    "    category_names = list(category_embeddings.keys())\n",
    "    category_vectors = np.array(list(category_embeddings.values()))\n",
    "    similarities = cosine_similarity([abstract_embedding], category_vectors)[0]\n",
    "    best_category_index = np.argmax(similarities)\n",
    "    assigned_category = category_names[best_category_index]\n",
    "    return assigned_category, similarities[best_category_index]\n",
    "\n",
    "word2vec_results = []\n",
    "\n",
    "def process_period_word2vec(period, df):\n",
    "    print(f\"Processing period: {period} with Word2Vec\")\n",
    "    concatenated_abstracts = \" \".join(df[\"Abstract\"].dropna())\n",
    "    if concatenated_abstracts:\n",
    "        concat_embedding = get_word2vec_embedding(concatenated_abstracts, word2vec_model)\n",
    "        unique_categories = df[\"Category\"].dropna().unique()\n",
    "        category_embeddings = get_category_embeddings_word2vec(unique_categories, word2vec_model)\n",
    "        \n",
    "        assigned_category, _ = assign_category(concat_embedding, category_embeddings)\n",
    "        compatible_count = sum(1 for cat in unique_categories if cat == assigned_category)\n",
    "        compatibility_ratio_concat = compatible_count / len(unique_categories) * 100 if unique_categories.size > 0 else 0\n",
    "        word2vec_results.append({\n",
    "            \"Period\": period,\n",
    "            \"Embedding\": \"Word2Vec\",\n",
    "            \"Method\": \"Concatenated\",\n",
    "            \"Compatibility (%)\": compatibility_ratio_concat\n",
    "        })\n",
    "    abstract_embeddings = []\n",
    "    actual_categories = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notnull(row[\"Abstract\"]):\n",
    "            abstract_embedding = get_word2vec_embedding(row[\"Abstract\"], word2vec_model)\n",
    "            assigned_category, _ = assign_category(abstract_embedding, category_embeddings)\n",
    "            abstract_embeddings.append(assigned_category)\n",
    "            actual_categories.append(row[\"Category\"])\n",
    "    compatible_count = sum(1 for i, actual_cat in enumerate(actual_categories) if actual_cat == abstract_embeddings[i])\n",
    "    compatibility_ratio_individual = compatible_count / len(abstract_embeddings) * 100 if abstract_embeddings else 0\n",
    "    word2vec_results.append({\n",
    "        \"Period\": period,\n",
    "        \"Embedding\": \"Word2Vec\",\n",
    "        \"Method\": \"Individual\",\n",
    "        \"Compatibility (%)\": compatibility_ratio_individual\n",
    "    })\n",
    "for period, df in all_dataframes.items():\n",
    "    process_period_word2vec(period, df)\n",
    "\n",
    "distilbert_concat_results = [{\"Period\": period, \"Embedding\": \"DistilBERT\", \"Method\": \"Concatenated\", \"Compatibility (%)\": ratio} for period, ratio in compatibility_results.items()]\n",
    "distilbert_individual_results = [{\"Period\": period, \"Embedding\": \"DistilBERT\", \"Method\": \"Individual\", \"Compatibility (%)\": ratio} for period, ratio in compatibility_results_individual.items()]\n",
    "\n",
    "word2vec_df = pd.DataFrame(word2vec_results)\n",
    "distilbert_concat_df = pd.DataFrame(distilbert_concat_results)\n",
    "distilbert_individual_df = pd.DataFrame(distilbert_individual_results)\n",
    "combined_df = pd.concat([word2vec_df, distilbert_concat_df, distilbert_individual_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nSummary Table of Compatibility Ratios for DistilBERT vs Word2Vec:\")\n",
    "print(combined_df)\n",
    "avg_compatibility = combined_df.groupby([\"Embedding\", \"Method\"])[\"Compatibility (%)\"].mean()\n",
    "print(\"\\nAverage Compatibility Ratios for Each Embedding and Method:\")\n",
    "print(avg_compatibility)\n",
    "results_df_pivot = combined_df.pivot(index=\"Period\", columns=[\"Embedding\", \"Method\"], values=\"Compatibility (%)\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "results_df_pivot.plot(kind=\"bar\", figsize=(12, 8), width=0.8)\n",
    "plt.title(\"Compatibility Ratios for DistilBERT vs Word2Vec by Period and Method\")\n",
    "plt.ylabel(\"Compatibility (%)\")\n",
    "plt.xlabel(\"Period\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Embedding & Method\")\n",
    "plt.savefig(\"compatibility_ratios_distilbert_vs_word2vec.svg\", dpi=300, bbox_inches='tight', format='svg')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
